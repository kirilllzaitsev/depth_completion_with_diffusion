from monocular depth estimation paper
    timesteps
        "this isn't discrete timesteps, choose whatever. 250 is good"
        training 2000
        inference 1000
    lr
        3e-5 during fine-tuning
        1e-4 during training
        learning rate warm-up over 10k steps
    batch size 64

training on BIG dataset, fine-tuning on SMALL dataset!!!

In config B, we re-adjust the basic hyperparameters to enable faster training and obtain a more
meaningful point of comparison. Specifically, we increase the parallelism from 4 to 8 GPUs and
batch size from 128 to 512 or 256, depending on the resolution. We also disable gradient clipping,
i.e., forcing kdL(Dθ)/dθk2 ≤ 1, that we found to provide no benefit in practice. Furthermore, we
increase the learning rate from 0.0002 to 0.001 for CIFAR-10, ramping it up during the first 10
million images, and standardize the half-life of the exponential moving average of θ to 0.5 million
images. Finally, we adjust the dropout probability for each dataset as shown in Table 7 via a full grid
search at 1% increments. Our total training time is approximately 2 days for CIFAR-10 at 32×32
resolution and 4 days for FFHQ and AFHQv2 at 64×64 resolution.

warump scheduler
    train_scheduler = CosineAnnealingLR(optimizer, num_epochs)

    def warmup(current_step: int):
        return 1 / (10 ** (float(number_warmup_epochs - current_step)))
    warmup_scheduler = LambdaLR(optimizer, lr_lambda=warmup)

    scheduler = SequentialLR(optimizer, [warmup_scheduler, train_scheduler], [number_warmup_epochs])