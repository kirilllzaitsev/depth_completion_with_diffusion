{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a9c4387b644d3bbe691314ac79852a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/39593d5650112b4cc580433f6b0435385882d819/text_encoder/pytorch_model.bin to /media/master/text/cv_data/hf_home/hub/tmpiaqtzlul\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4affe6887c094a82a709b9862206eca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Storing https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/39593d5650112b4cc580433f6b0435385882d819/text_encoder/pytorch_model.bin in cache at /media/master/text/cv_data/hf_home/hub/models--runwayml--stable-diffusion-v1-5/blobs/770a47a9ffdcfda0b05506a7888ed714d06131d60267e6cf52765d61cf59fd67\n",
      "Creating pointer from ../../../blobs/770a47a9ffdcfda0b05506a7888ed714d06131d60267e6cf52765d61cf59fd67 to /media/master/text/cv_data/hf_home/hub/models--runwayml--stable-diffusion-v1-5/snapshots/39593d5650112b4cc580433f6b0435385882d819/text_encoder/pytorch_model.bin\n",
      "/home/master/.conda/envs/ssdc/lib/python3.10/site-packages/transformers-4.28.1-py3.10.egg/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0606ed9dd34f27bafa3aa15792d541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "device = \"cuda\"\n",
    "model_id_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id_or_path, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
    "\n",
    "response = requests.get(url)\n",
    "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "init_image = init_image.resize((768, 512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_image=Image.open(\"/media/master/wext/cv_data/kitti-full/data/kitti_depth_completion/testing/image/0000000000.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5da0f90bdd4c63a7ebfa3954953620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# prompt = \"A fantasy landscape, trending on artstation\"\n",
    "# prompt = \"An urban landscape\"\n",
    "prompt_embeds=torch.randn(1,4,768)\n",
    "images = pipe(\n",
    "    prompt=prompt, \n",
    "    # prompt_embeds=prompt_embeds,\n",
    "              image=init_image, strength=0.75, guidance_scale=7.5).images\n",
    "images[0].save(\"debug/fantasy_landscape.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "from diffusers import UNet2DConditionModel, StableDiffusionImg2ImgPipeline, AutoencoderKL\n",
    "import torch\n",
    "\n",
    "vae = AutoencoderKL(in_channels=1, out_channels=1)\n",
    "model_id_or_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    model_id_or_path, unet=model, vae=vae, #torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL(in_channels=3, out_channels=1, latent_channels=1)\n",
    "unet = UNet2DConditionModel(\n",
    "    sample_size=64,  # the target image resolution\n",
    "    in_channels=1,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=1,  # the number of output channels\n",
    "    layers_per_block=1,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(\n",
    "        128,\n",
    "        256,\n",
    "        512,\n",
    "    ),  # the number of output channels for each UNet block\n",
    "    cross_attention_dim=512,\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",\n",
    "        \"CrossAttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"CrossAttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")\n",
    "pipeline.vae=vae\n",
    "pipeline.unet=unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/ssdc/lib/python3.10/site-packages/diffusers/image_processor.py:154: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-4.08251428604126,3.788675546646118]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1122a6361e924f219ecedd8d5a48b66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = torch.randn(1, 3, 64, 64)#.to(torch.float16)\n",
    "encoder_hidden_states = torch.randn(1, 1, 512)#.to(torch.float16)\n",
    "samples=pipeline(\n",
    "    # batch_size=config.eval_batch_size,\n",
    "    image=image,\n",
    "    generator=torch.manual_seed(1),\n",
    "    num_inference_steps=20,\n",
    "    output_type=\"numpy\",\n",
    "    prompt_embeds=encoder_hidden_states,\n",
    "    negative_prompt_embeds=torch.zeros_like(encoder_hidden_states),\n",
    ").images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
