{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import comet_ml\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from rsl_depth_completion.diffusion.utils import set_seed\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "from kbnet import data_utils\n",
    "import yaml\n",
    "import argparse\n",
    "from utils import plot_sample\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from torchvision.utils import save_image\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 1\n",
    "timesteps = 300\n",
    "# timesteps = 30\n",
    "\n",
    "seed = 100\n",
    "set_seed(seed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device='cpu'\n",
    "\n",
    "from config import tmpdir\n",
    "is_cluster = os.path.exists('/cluster')\n",
    "if is_cluster:\n",
    "    if not os.path.exists(f'{tmpdir}/cluster'):\n",
    "        !tar -xvf /cluster/project/rsl/kzaitsev/datasets.tar -C $TMPDIR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_model_ref = \"openai/clip-vit-base-patch32\"\n",
    "extractor_model = CLIPModel.from_pretrained(extractor_model_ref)\n",
    "extractor_processor = CLIPProcessor.from_pretrained(extractor_model_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import path_to_project_dir, base_kitti_dataset_dir\n",
    "\n",
    "ds_config_str = open(\n",
    "    f\"{path_to_project_dir}/rsl_depth_completion/configs/data/kitti_custom.yaml\"\n",
    ").read()\n",
    "ds_config_str = ds_config_str.replace(\"${data_dir}\", base_kitti_dataset_dir)\n",
    "ds_config = argparse.Namespace(**yaml.safe_load(ds_config_str)[\"ds_config\"])\n",
    "ds_config.use_pose = \"photo\" in ds_config.train_mode\n",
    "ds_config.result = ds_config.result_dir\n",
    "ds_config.use_rgb = (\"rgb\" in ds_config.input) or ds_config.use_pose\n",
    "ds_config.use_d = \"d\" in ds_config.input\n",
    "ds_config.use_g = \"g\" in ds_config.input\n",
    "val_image_paths = data_utils.read_paths(ds_config.val_image_path)\n",
    "val_sparse_depth_paths = data_utils.read_paths(ds_config.val_sparse_depth_path)\n",
    "val_intrinsics_paths = data_utils.read_paths(ds_config.val_intrinsics_path)\n",
    "val_ground_truth_paths = data_utils.read_paths(ds_config.val_ground_truth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from rsl_depth_completion.conditional_diffusion import utils\n",
    "from rsl_depth_completion.data.kitti.kitti_dataset import CustomKittiDCDataset\n",
    "\n",
    "\n",
    "class MinimagenDatasetCustom(CustomKittiDCDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        include_cond_image=False,\n",
    "        sdm_transform=None,\n",
    "        do_crop=False,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.include_cond_image = include_cond_image\n",
    "        self.default_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "            ]\n",
    "        )\n",
    "        self.sdm_transform = sdm_transform or self.default_transform\n",
    "        self.max_depth = 80\n",
    "        self.do_crop = do_crop\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        items = super().__getitem__(idx)\n",
    "        if self.do_crop:\n",
    "            items[\"d\"] = items[\"d\"][:, 50 : 50 + 256, 400 : 400 + 256]\n",
    "            items[\"img\"] = items[\"img\"][:, 50 : 50 + 256, 400 : 400 + 256]\n",
    "        sparse_dm = items[\"d\"]\n",
    "        sparse_dm /= self.max_depth\n",
    "\n",
    "        interpolated_sparse_dm = torch.from_numpy(\n",
    "            # utils.infill_sparse_depth(sparse_dm.numpy())\n",
    "            utils.interpolate_sparse_depth(\n",
    "                sparse_dm.squeeze().numpy(), do_multiscale=True\n",
    "            )\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        rgb_image = items[\"img\"]\n",
    "\n",
    "        rgb_pixel_values = self.extract_img_features(rgb_image)\n",
    "        sdm_pixel_values = self.extract_img_features(\n",
    "            cv2.cvtColor(sparse_dm.squeeze().numpy(), cv2.COLOR_GRAY2RGB)\n",
    "        )\n",
    "        rgb_embed = extractor_model.get_image_features(pixel_values=rgb_pixel_values)\n",
    "        sdm_embed = extractor_model.get_image_features(pixel_values=sdm_pixel_values)\n",
    "\n",
    "        sample = {\n",
    "            \"perturbed_sdm\": interpolated_sparse_dm.detach(),\n",
    "            \"rgb_embed\": rgb_embed.detach(),\n",
    "            \"sdm_embed\": sdm_embed.detach(),\n",
    "            \"rgb_image\": (rgb_image / 255).detach(),\n",
    "            \"sparse_dm\": (sparse_dm).detach(),\n",
    "        }\n",
    "        return sample\n",
    "\n",
    "    def extract_img_features(self, cond_image):\n",
    "        return extractor_processor(\n",
    "            images=torch.stack(\n",
    "                [\n",
    "                    torch.from_numpy(np.array(cond_image)),\n",
    "                ]\n",
    "            ),\n",
    "            return_tensors=\"pt\",\n",
    "        ).pixel_values\n",
    "\n",
    "\n",
    "ds = MinimagenDatasetCustom(\n",
    "    ds_config=ds_config,\n",
    "    image_paths=val_image_paths,\n",
    "    sparse_depth_paths=val_sparse_depth_paths,\n",
    "    intrinsics_paths=val_intrinsics_paths,\n",
    "    ground_truth_paths=val_ground_truth_paths,\n",
    "    include_cond_image=True,\n",
    "    do_crop=True,\n",
    ")\n",
    "x = ds[0]\n",
    "plot_sample(x)\n",
    "x[\"perturbed_sdm\"].shape, x[\"rgb_embed\"].shape, x[\"sdm_embed\"].shape, x[\n",
    "    \"rgb_image\"\n",
    "].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_subset = torch.utils.data.Subset(\n",
    "    ds,\n",
    "    range(0, len(ds) // 2)\n",
    "    # range(0, 5)\n",
    ")\n",
    "train_size = int(0.8 * len(ds_subset))\n",
    "test_size = len(ds_subset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(\n",
    "    ds_subset, [train_size, test_size]\n",
    ")\n",
    "is_cluster = os.path.exists(\"/cluster\")\n",
    "if is_cluster:\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_WORKERS = min(20, BATCH_SIZE)\n",
    "else:\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "dl_opts = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"drop_last\": True,\n",
    "    # \"collate_fn\": MinimagenCollator(device),\n",
    "}\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, **dl_opts, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, **dl_opts, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader), len(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imagen_pytorch import Unet, Imagen\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "unet_base_params = dict(\n",
    "    dim=64,\n",
    "    dim_mults=[1, 1, 2, 2, 4, 4],\n",
    "    channels=1,\n",
    "    channels_out=None,\n",
    "    text_embed_dim=512,\n",
    "    num_resnet_blocks=2,\n",
    "    layer_attns=[False, False, False, False, False, True],\n",
    "    layer_cross_attns=[False, False, False, False, False, True],\n",
    "    attn_heads=8,\n",
    "    lowres_cond=False,\n",
    "    memory_efficient=False,\n",
    "    attend_at_middle=False,\n",
    "    cond_dim=None,\n",
    "    cond_images_channels=3,\n",
    ")\n",
    "imagen_params = dict(\n",
    "    text_embed_dim=512,\n",
    "    channels=1,\n",
    "    timesteps=timesteps,\n",
    "    loss_type=\"l2\",\n",
    "    lowres_sample_noise_level=0.2,\n",
    "    dynamic_thresholding_percentile=0.9,\n",
    "    only_train_unet_number=None,\n",
    "    image_sizes=[128],\n",
    "    text_encoder_name=\"google/t5-v1_1-base\",\n",
    "    auto_normalize_img=True,\n",
    "    cond_drop_prob=0.1,\n",
    "    condition_on_text=True,\n",
    ")\n",
    "\n",
    "unet_base = Unet(**unet_base_params)\n",
    "unets = [unet_base]\n",
    "\n",
    "\n",
    "imagen = Imagen(unets=unets, **imagen_params)\n",
    "\n",
    "unet_base.to(device)\n",
    "imagen.to(device)\n",
    "\n",
    "imagen = torch.compile(imagen)\n",
    "\n",
    "print(\n",
    "    \"Number of parameters in model\",\n",
    "    sum(p.numel() for p in imagen.parameters() if p.requires_grad),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 5e-6\n",
    "optimizer = optim.Adam(imagen.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = comet_ml.Experiment(\n",
    "    api_key=\"W5npcWDiWeNPoB2OYkQvwQD0C\",\n",
    "    project_name=\"rsl_depth_completion\",\n",
    "    auto_metric_logging=True,\n",
    "    auto_param_logging=True,\n",
    "    auto_histogram_tensorboard_logging=True,\n",
    "    log_env_details=False,\n",
    "    log_env_host=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.log_parameters({f\"imagen_{k}\": v for k, v in imagen_params.items()})\n",
    "experiment.log_parameters({f\"unet_base_{k}\": v for k, v in unet_base_params.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = Path(\"./logs\")\n",
    "input_name = \"interp_sdm\"\n",
    "cond = \"rgb+sdm\"\n",
    "exp_dir = f\"{input_name=}/{cond=}/{lr=}_{timesteps=}\"\n",
    "train_logdir = logdir / \"train\" / exp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 21\n",
    "progress_bar = tqdm(total=num_epochs, disable=False)\n",
    "out_dir = f\"results/{exp_dir}\"\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "train_writer = tf.summary.create_file_writer(str(train_logdir))\n",
    "\n",
    "start_epoch_scaler = 0\n",
    "start_epoch = num_epochs * start_epoch_scaler\n",
    "final_epoch = start_epoch + num_epochs * 2\n",
    "\n",
    "for epoch in range(start_epoch, final_epoch):\n",
    "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "    optimizer.zero_grad()\n",
    "    running_loss = {\"loss\": 0}\n",
    "    with torch.autocast(device.type):\n",
    "        for batch_idx, batch in tqdm(\n",
    "            enumerate(train_dataloader), total=len(train_dataloader)\n",
    "        ):\n",
    "            perturbed_sdm = batch[\"perturbed_sdm\"].to(device)\n",
    "            rgb_img = batch[\"rgb_image\"].to(device)\n",
    "            sdm_embed = batch[\"sdm_embed\"].to(device)\n",
    "            for i in range(1, 2):\n",
    "                loss = imagen(\n",
    "                    perturbed_sdm,\n",
    "                    text_embeds=sdm_embed,\n",
    "                    cond_images=rgb_img,\n",
    "                    unet_number=i,\n",
    "                )\n",
    "                loss.backward()\n",
    "                running_loss[\"loss\"] += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.scalar(\n",
    "                    \"batch/loss\",\n",
    "                    loss.item(),\n",
    "                    step=epoch * len(train_dataloader) + batch_idx,\n",
    "                )\n",
    "\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar(\"epoch/loss\", running_loss[\"loss\"], step=epoch)\n",
    "\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    if (epoch - 1) % 5 == 0:\n",
    "        print(f\"Epoch: {epoch}\\t{running_loss}\")\n",
    "\n",
    "        progress_bar.set_postfix(**running_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            samples = imagen.sample(\n",
    "                text_embeds=sdm_embed,\n",
    "                cond_images=rgb_img,\n",
    "                cond_scale=1.0,\n",
    "                stop_at_unet_number=1,\n",
    "                return_all_unet_outputs=True,\n",
    "            )\n",
    "\n",
    "        first_sample_in_batch = samples[0][0].permute(1, 2, 0).cpu().detach().numpy()\n",
    "        plt.imshow(first_sample_in_batch)\n",
    "        plt.savefig(f\"{out_dir}/first_sample_in_batch_{epoch:04d}.png\")\n",
    "        out_path = f\"{out_dir}/sample-{epoch}.png\"\n",
    "        save_image(samples[0], str(out_path), nrow=10)\n",
    "        out_img = Image.open(out_path)\n",
    "        max_outputs = len(samples[0])\n",
    "        with train_writer.as_default():\n",
    "            tf.summary.image(\n",
    "                \"samples\",\n",
    "                np.expand_dims(np.array(out_img), 0),\n",
    "                max_outputs=max_outputs,\n",
    "                step=epoch,\n",
    "            )\n",
    "            tf.summary.image(\n",
    "                \"interp_sdm\",\n",
    "                batch[\"perturbed_sdm\"].numpy().transpose(0, 2, 3, 1),\n",
    "                max_outputs=max_outputs,\n",
    "                step=epoch,\n",
    "            )\n",
    "            tf.summary.image(\n",
    "                \"sdm\",\n",
    "                batch[\"sparse_dm\"].numpy().transpose(0, 2, 3, 1),\n",
    "                max_outputs=max_outputs,\n",
    "                step=epoch,\n",
    "            )\n",
    "            tf.summary.image(\n",
    "                \"rgb\",\n",
    "                batch[\"rgb_image\"].numpy().transpose(0, 2, 3, 1),\n",
    "                max_outputs=max_outputs,\n",
    "                step=epoch,\n",
    "            )\n",
    "\n",
    "torch.save(imagen.state_dict(), f\"{out_dir}/imagen_epoch_{final_epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssdc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
